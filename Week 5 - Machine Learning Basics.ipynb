{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "- Machine learning (ML) is a branch of artificial intelligence (AI) focused on enabling computers and machines to imitate the way that humans learn\n",
    "- ML helps to perform tasks autonomously, and to improve their performance and accuracy through experience and exposure to more data\n",
    "## Types of Machine Learning\n",
    "- Supervised Machine Learning\n",
    "- Unsupervised Machine Learning\n",
    "- Reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Machine Learning\n",
    "- It is a machine learning technique that uses labeled datasets to train artificial intelligence algorithm models to identify the underlying patterns and relationships between input features and outputs\n",
    "- There are mainly two sub-sections of supervised Machine Learning\n",
    "\n",
    "### Regression\n",
    "Regression is used for predicting the continuous value\n",
    "- Simple Linear Regression\n",
    "- Multiple Linear Regression\n",
    "- Polynomial Regression\n",
    "### Classification\n",
    "Classification is for predicting the categorial (discreate) values (e.g. Yes/No, image classification etc)\n",
    "- Classical Machine Learning Algorithms\n",
    "    - Naive Baye's Classifier: (GNB)\n",
    "    - Decision Tree Classifier\n",
    "    - Random Forest Classifier\n",
    "    - Support Vector Machine (SVM)\n",
    "    - Logistic Regression: Used for binary classification problems\n",
    "\n",
    "- Neural Networks: Deep Learning based classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n",
    "Simple Linear Regression only has one in-dependent variable (x) and one dependent variable (y). \n",
    "- x input\n",
    "- y target\n",
    "\n",
    "We use the equation of Straight line to find out the value of dependent variable by\n",
    "``x = mx +c``\n",
    "\n",
    "**Example**\n",
    "Below is the data for hours student studied and the marks obtained\n",
    "\n",
    "<img src=\"./assets/linear-regression.jpg\" alt=\"simple linear regression\" style=\"width: 700px; height: auto;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve the Linear Regression and Predict the marks for students who studied 6 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Marks when hours of studied is 6 = ?\n"
     ]
    }
   ],
   "source": [
    "# Predict the marks when hours of studied = 6\n",
    "x = 6\n",
    "print(f\" Marks when hours of studied is 6 = ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW -  **Predict Demand using the given data**\n",
    "| Price | Demand |\n",
    "|-------|---------|\n",
    "|   10\t    |   500    |\n",
    "|   15\t    |   450    |\n",
    "|   20\t    |   400    |\n",
    "|   25\t    |   350   | \n",
    "|   30\t    |   300   | \n",
    "\n",
    "- Find the linear equation for above data (D=mP+c) where price is independent variable (P) and demand is dependable variable (D)\n",
    "- Predict the Demand when Price is 37.6 \n",
    "- Attach the Handwrittern screenshot of your solution\n",
    "- Also solve the problem using Python (with pandas or numpy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "### Naive Bayes Classifier\n",
    "- The Naive Bayes classifier uses Bayes Theorem to solve the classification problems by computing class probabilities of the given feature values.\n",
    "- It is used for Spam Detection, Sentiment Analysis and Text Classification etc.\n",
    "\n",
    "### Naive Bayes Theorem\n",
    "$$\n",
    "P(H|E) = \\frac{P(E|H) \\cdot P(H)}{P(E)}\n",
    "$$\n",
    "Where:\n",
    "\n",
    "- \\( P(H|E) \\) = **Posterior probability** (Probability of hypothesis \\( H \\) given evidence \\( E \\))\n",
    "- \\( P(E|H) \\) = **Likelihood** (Probability of evidence \\( E \\) given hypothesis \\( H \\))\n",
    "- \\( P(H) \\) = **Prior probability** (Initial probability of hypothesis \\( H \\))\n",
    "- \\( P(E) \\) = **Evidence probability** (Overall probability of evidence \\( E \\))\n",
    "\n",
    "### Example of Naive Bayes Theorem\n",
    "If the test result for a dieses is +ve what is the probability of having the disease based on the historical data\n",
    "\n",
    "| Disease  | Test Positive | Test Negative | Total |\n",
    "|----------|--------------|--------------|-------|\n",
    "| Yes (D)  | 40           | 10           | 50    |\n",
    "| No (~D)  | 30           | 120          | 150   |\n",
    "| **Total** | 70           | 130          | 200   |\n",
    "\n",
    "\n",
    "<img src=\"./assets/Bayes-theorem.jpg\" alt=\"bayes theorem\" style=\"width: 700px; height: auto;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **HW - Create a  function in Python that that accepts above historical table as a Pandas DataFrame and calculate the probability of having disease if tested positive using Bayes Theorem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier\n",
    "\n",
    "Let us suppose we have our the following sentence from our email and also the label for email as \"Spam\" or \"Not\"\n",
    "\n",
    "|Text   | Label|\n",
    "|-------|------|\n",
    "|Win a free prize now|Spam|\n",
    "|Meeting at 5PM tomorrow|Not Spam|\n",
    "|Claim your discount today| Spam|\n",
    "|Reminder: Project deadline| Not Spam|\n",
    "\n",
    "Now we have to predict that email containing **Win a discount today** is spam or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1- Represent the above data into pandas data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_data = pd.DataFrame({\n",
    "    'text':['Win a free prize now','Meeting at 5PM tomorrow','Claim your discount today','Reminder: Project deadline'],\n",
    "    'label':['Spam','Not Spam','Spam','Not Spam']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 Calculate the Probablity of email being spam if it contains the given words (features)\n",
    "$$\n",
    "P(\\text{Spam} \\mid \\text{Features}) = \\frac{P(\\text{Features} \\mid \\text{Spam}) \\cdot P(\\text{Spam})}{P(\\text{Features})}\n",
    "$$\n",
    "\n",
    "- For this lets consider each words as Features and represent each sentence as boolean vector\n",
    "​\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>sentence_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Win a free prize now</td>\n",
       "      <td>Spam</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Meeting at 5PM tomorrow</td>\n",
       "      <td>Not Spam</td>\n",
       "      <td>[1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Claim your discount today</td>\n",
       "      <td>Spam</td>\n",
       "      <td>[0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Reminder: Project deadline</td>\n",
       "      <td>Not Spam</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         text     label  \\\n",
       "0        Win a free prize now      Spam   \n",
       "1     Meeting at 5PM tomorrow  Not Spam   \n",
       "2   Claim your discount today      Spam   \n",
       "3  Reminder: Project deadline  Not Spam   \n",
       "\n",
       "                                    sentence_vector  \n",
       "0  [0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0]  \n",
       "1  [1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0]  \n",
       "2  [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1]  \n",
       "3  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract all unique features (words)\n",
    "all_features  = set()\n",
    "for text in email_data['text']:\n",
    "    words = text.lower().split()\n",
    "    all_features.update(words)\n",
    "\n",
    "all_features = sorted(list(all_features))\n",
    "\n",
    "def create_binary_vector(sentence):\n",
    "    words = sentence.lower().split()  \n",
    "    return [1 if word in words else 0 for word in all_features]  \n",
    "\n",
    "email_data['sentence_vector'] = email_data['text'].apply(create_binary_vector)\n",
    "email_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Apply Naive Bayes Classifier to predict the email containing the sentence \"**Win a discount today**\" is spam or not\n",
    "\n",
    "Sub-Steps\n",
    "##### Step 1: Calculate Prior Probabilities\n",
    "##### Step 2: Calculate the Likelihoods\n",
    "##### Step 3: Use Bayes' Theorem** to **Classify the Sentence \"Win a discount today\"\n",
    "Using the likelihoods and prior probabilities ,  calculate the posterior probabilities for Spam and Not Spam:\n",
    "$$ P(\\text{Spam} \\mid \\text{Features}) \\propto P(\\text{Spam}) \\cdot \\prod_{i=1}^{n} P(\\text{word}_i \\mid \\text{Spam})$$\n",
    "$$ P(\\text{Not Spam} \\mid \\text{Features}) \\propto P(\\text{Not Spam}) \\cdot \\prod_{i=1}^{n} P(\\text{word}_i \\mid \\text{Not Spam}) $$\n",
    "\n",
    "After calculating both values,  choose the class with the higher posterior probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate prior probabilities\n",
    "spam_count = len(email_data[email_data['label'] == 'Spam'])\n",
    "not_spam_count = len(email_data[email_data['label'] == 'Not Spam'])\n",
    "total_count = len(email_data)\n",
    "\n",
    "P_spam = spam_count / total_count\n",
    "P_not_spam = not_spam_count / total_count\n",
    "\n",
    "\n",
    "# Calculate likelihoods for each feature (word) given Spam and Not Spam\n",
    "spam_data = email_data[email_data['label'] == 'Spam']\n",
    "not_spam_data = email_data[email_data['label'] == 'Not Spam']\n",
    "\n",
    "\n",
    "# Initialize dictionaries to hold probabilities\n",
    "P_word_given_spam = {}\n",
    "P_word_given_not_spam = {}\n",
    "\n",
    "# For each word, calculate P(word|Spam) and P(word|Not Spam)\n",
    "for word in all_features:\n",
    "    P_word_given_spam[word] = (spam_data['sentence_vector'].apply(lambda x: x[all_features.index(word)]).sum() + 1) / (len(spam_data) + 2)\n",
    "    P_word_given_not_spam[word] = (not_spam_data['sentence_vector'].apply(lambda x: x[all_features.index(word)]).sum() + 1) / (len(not_spam_data) + 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict for new sentence (Win a discount today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence is classified as Spam.\n"
     ]
    }
   ],
   "source": [
    "# New sentence to classify\n",
    "new_sentence = \"Win a discount today\"\n",
    "new_vector = create_binary_vector(new_sentence)\n",
    "\n",
    "# Calculate P(Spam | Features) and P(Not Spam | Features)\n",
    "P_spam_given_features = P_spam\n",
    "P_not_spam_given_features = P_not_spam\n",
    "\n",
    "for i, word in enumerate(all_features):\n",
    "    if new_vector[i] == 1:\n",
    "        P_spam_given_features *= P_word_given_spam[word]\n",
    "        P_not_spam_given_features *= P_word_given_not_spam[word]\n",
    "    else:\n",
    "        P_spam_given_features *= (1 - P_word_given_spam[word])\n",
    "        P_not_spam_given_features *= (1 - P_word_given_not_spam[word])\n",
    "\n",
    "# Normalize the results\n",
    "total_prob = P_spam_given_features + P_not_spam_given_features\n",
    "P_spam_given_features /= total_prob\n",
    "P_not_spam_given_features /= total_prob\n",
    "\n",
    "\n",
    "if P_spam_given_features > P_not_spam_given_features:\n",
    "    print(\"The sentence is classified as Spam.\")\n",
    "else:\n",
    "    print(\"The sentence is classified as Not Spam.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier\n",
    "<img src=\"./assets/decision-tree.png\" alt=\"Decision Tree\" style=\"width: 400px; height: auto;\">\n",
    "\n",
    "\n",
    "| Day   | Rain | Cloudy | Bring Umbrella? |\n",
    "|-------|------|--------|-----------------|\n",
    "| Day 1 | Yes  | Yes    | Yes             |\n",
    "| Day 2 | Yes  | No     | Yes             |\n",
    "| Day 3 | No   | Yes    | Yes             |\n",
    "| Day 4 | No   | No     | No              |\n",
    "| Day 5 | Yes  | Yes    | Yes             |\n",
    "\n",
    "\n",
    "\n",
    "- **Training**: The tree learns from past data.\n",
    "- It tries to split the data into different categories (e.g., \"Bring Umbrella: Yes\" or \"No\") by asking questions that best separate the data points.\n",
    "- Classifying new data: When we get new weather data, the decision tree will ask the same questions:\n",
    "\n",
    "Is it raining? → If yes, the tree says bring an umbrella."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematics Behind Decision Tree\n",
    "- In Decision Tree Classifier we split the data such data the information Gain is optimal\n",
    "- For optimal Information Gain we use Entropy\n",
    "\n",
    "<img src=\"./assets/better-split.png\" alt=\"Decision Tree\" style=\"width: 300px; height: auto;\">\n",
    "<img src=\"./assets/Decision-tree.png\" alt=\"Decision Tree\" style=\"width: 400px; height: auto;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$ Entropy(S) = - \\sum_{i=1}^{c} p_i \\log_2 p_i $$\n",
    "\n",
    "$$ IG(S, A) = Entropy(S) - \\sum_{i} \\omega_i Entropy(S_i) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier\n",
    "<img src=\"./assets/random-forest.jpg\" alt=\"Random Forest\" style=\"width: 500px; height: auto;\">\n",
    "\n",
    "- Random Forest is an ensemble machine learning technique that builds multiple decision trees and combines their results to improve accuracy \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Classifier\n",
    "- Logistic Regression is similar to Linear Regression, but it predicts a probability and classifies data into categories like Yes/No 1,0 etc.\n",
    "\n",
    "##### Working\n",
    "- Predict the output based on input as like Linear regression \n",
    "- y = mx +c\n",
    "\n",
    "- then apply the Sigmoid function on y to find the probablity\n",
    "- if Probablity is greater than/equal to 0.5 its Yes, otherwise is no\n",
    "\n",
    "$$\n",
    "LR(y) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Now decision is base don the LR(y) as \n",
    "$$\n",
    "\\text{If } P(Y=1) > 0.5 \\Rightarrow \\text{ Predict Class } 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{If } P(Y=1) \\leq 0.5 \\Rightarrow \\text{ Predict Class } 0\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning\n",
    "- Lets create a Neural Network with \n",
    "    - One Input Neuron\n",
    "    - One Hidden Layer\n",
    "    - One Output Neuron\n",
    "\n",
    "- Train this Neural Network to find the the pattern in our dataset\n",
    "- Out dataset contains \n",
    "    - x=Investment On Stock Market\n",
    "    - y= Return (= x*3)\n",
    "- Train The Neural Network and later predict the Return for new investment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.39935544]] [0.] [[0.92463368]] [0.]\n",
      "Epoch 0, Loss: 5.5317\n",
      "Epoch 100, Loss: 1.8797\n",
      "Epoch 200, Loss: 1.1632\n",
      "Epoch 300, Loss: 0.7821\n",
      "Epoch 400, Loss: 0.5618\n",
      "Epoch 500, Loss: 0.4258\n",
      "Epoch 600, Loss: 0.3359\n",
      "Epoch 700, Loss: 0.2741\n",
      "Epoch 800, Loss: 0.2299\n",
      "Epoch 900, Loss: 0.1971\n",
      "Input: 2.0, Predicted: 6.0045, Expected: 6.0000\n",
      "Input: -1.5, Predicted: -4.2607, Expected: -4.5000\n",
      "Input: 0.0, Predicted: -0.0115, Expected: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42) \n",
    "x_train = np.random.normal(0, 1, (1000, 1))\n",
    "y_train = 3 * x_train \n",
    "\n",
    "# Initialize weights and biases\n",
    "w1 = np.random.randn(1, 1) \n",
    "b1 = np.zeros((1,)) \n",
    "w2 = np.random.randn(1, 1)  \n",
    "b2 = np.zeros((1,)) \n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.01\n",
    "epochs = 1000\n",
    "print(w1,b1,w2,b2)\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    hidden_input = np.dot(x_train, w1) + b1\n",
    "    hidden_output = relu(hidden_input)  # Hidden layer activation\n",
    "    \n",
    "    output = np.dot(hidden_output, w2) + b2  # Output layer (linear activation)\n",
    "    \n",
    "    # Compute loss (Mean Squared Error)\n",
    "    loss = np.mean((output - y_train) ** 2)\n",
    "\n",
    "    # Backpropagation\n",
    "    d_loss_output = 2 * (output - y_train) / len(y_train)  # dL/dy\n",
    "    d_w2 = np.dot(hidden_output.T, d_loss_output)  # Gradient for w2\n",
    "    d_b2 = np.sum(d_loss_output, axis=0)  # Gradient for b2\n",
    "\n",
    "    d_hidden = np.dot(d_loss_output, w2.T) * relu_derivative(hidden_input)  # Backprop through ReLU\n",
    "    d_w1 = np.dot(x_train.T, d_hidden)  # Gradient for w1\n",
    "    d_b1 = np.sum(d_hidden, axis=0)  # Gradient for b1\n",
    "\n",
    "    # Update weights and biases\n",
    "    w1 -= lr * d_w1\n",
    "    b1 -= lr * d_b1\n",
    "    w2 -= lr * d_w2\n",
    "    b2 -= lr * d_b2\n",
    "\n",
    "    # Print loss every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Testing the trained model\n",
    "x_test = np.array([[2.0], [7.5], [0.0]])  # Test inputs\n",
    "hidden_input_test = np.dot(x_test, w1) + b1\n",
    "hidden_output_test = relu(hidden_input_test)\n",
    "y_pred = np.dot(hidden_output_test, w2) + b2  # Output layer prediction\n",
    "\n",
    "# Print results\n",
    "for i, x_val in enumerate(x_test):\n",
    "    print(f\"Input: {x_val[0]}, Predicted: {y_pred[i][0]:.4f}, Expected: {3 * x_val[0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.39004524]] [1.96361602] [[2.16395982]] [-4.2607315]\n"
     ]
    }
   ],
   "source": [
    "print(w1,b1,w2,b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assignment:\n",
    "- Use scikit-learn (https://scikit-learn.org/stable/) and use following algorithms for regression and Classification\n",
    "- Regression (Predict House Price) - Can use any publicly available data\n",
    "- GNB (Classify the Email as Spam or Not)\n",
    "- Decision Tree Classifier (Any classification Task)\n",
    "\n",
    "- Guidelines\n",
    "- Apply data cleaning and pre-processing \n",
    "- Prepare Test and Train Dataset\n",
    "- Train the Model with the processed Data\n",
    "- Use the Classifier/Regression and Calculate the Accuracy\n",
    "- Use the Trained Model for Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
